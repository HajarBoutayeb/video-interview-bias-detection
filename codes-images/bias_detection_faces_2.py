import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, 
                           classification_report, confusion_matrix, roc_auc_score, roc_curve)
from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTETomek, SMOTEENN
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# ================= CHEMINS =================
EXCEL_PATH = r"C:\Users\ADmiN\Desktop\video_project\annotations\faces_annotations.xlsx"

# ================= CONFIGURATION =================
plt.style.use('seaborn-v0_8')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

# Strat√©gies d'√©chantillonnage pour donn√©es d√©s√©quilibr√©es
SAMPLING_STRATEGIES = {
    'SMOTE': SMOTE(random_state=42),
    'ADASYN': ADASYN(random_state=42),
    'BorderlineSMOTE': BorderlineSMOTE(random_state=42),
    'SMOTETomek': SMOTETomek(random_state=42),
    'SMOTEENN': SMOTEENN(random_state=42)
}

# Classificateurs am√©lior√©s optimis√©s pour le genre et l'√¢ge
CLASSIFIERS = {
    "RandomForest": RandomForestClassifier(
        n_estimators=300, 
        max_depth=15,
        min_samples_split=3,
        min_samples_leaf=1,
        random_state=42,
        n_jobs=-1
    ),
    "GradientBoosting": GradientBoostingClassifier(
        n_estimators=300,
        learning_rate=0.08,
        max_depth=8,
        random_state=42
    ),
    "AdaBoost": AdaBoostClassifier(
        n_estimators=150,
        learning_rate=0.8,
        random_state=42
    ),
    "SVM": SVC(
        probability=True, 
        random_state=42,
        kernel='rbf',
        C=10.0,
        gamma='scale'
    ),
    "LogisticRegression": LogisticRegression(
        random_state=42,
        max_iter=2000,
        C=1.0,
        solver='lbfgs'
    ),
    "MLP": MLPClassifier(
        hidden_layer_sizes=(150, 100, 50),
        max_iter=1000,
        random_state=42,
        early_stopping=True,
        validation_fraction=0.15
    ),
    "NaiveBayes": GaussianNB()
}

# ================= CHARGER LES DONN√âES =================
print("üîÑ Chargement des donn√©es pour l'analyse Genre & √Çge...")
df = pd.read_excel(EXCEL_PATH)
print(f"üìä Taille initiale des donn√©es : {df.shape}")

# ================= √âTIQUETTES CIBLES =================
target_labels = ['gender', 'age_category']

print(f"üéØ √âtiquettes cibles : {target_labels}")
print("   - Genre : Classification Homme/Femme")
print("   - Cat√©gorie d'√Çge : Classification par groupe d'√¢ge")

# ================= CR√âER CAT√âGORIES D'√ÇGE =================
def get_age_category(age):
    if pd.isna(age):
        return "Unknown"
    if age < 13: return "Child"
    elif age < 20: return "Teen"
    elif age < 30: return "Young Adult"
    elif age < 45: return "Adult"
    elif age < 65: return "Middle-aged"
    else: return "Senior"

# Cr√©er la cat√©gorie d'√¢ge si elle n'existe pas
if 'age_category' not in df.columns:
    df['age_category'] = df['age'].apply(get_age_category)

# Supprimer les cat√©gories d'√¢ge inconnues pour un meilleur entra√Ænement
df = df[df['age_category'] != 'Unknown']

# ================= PR√âTRAITEMENT DES DONN√âES =================
def clean_numeric_columns(df, columns):
    """Nettoyer et convertir les colonnes en num√©riques"""
    cleaned_df = df.copy()
    
    for col in columns:
        if col in cleaned_df.columns:
            # Convertir en string d'abord, puis nettoyer
            cleaned_df[col] = cleaned_df[col].astype(str)
            # Remplacer les virgules par des points et supprimer les points finaux
            cleaned_df[col] = cleaned_df[col].str.replace(',', '.', regex=False).str.rstrip('.')
            # Remplacer 'nan', 'None', cha√Ænes vides avec NaN
            cleaned_df[col] = cleaned_df[col].replace(['nan', 'None', '', 'null'], np.nan)
            # Convertir en num√©rique
            cleaned_df[col] = pd.to_numeric(cleaned_df[col], errors='coerce')
            # Remplir NaN avec des valeurs appropri√©es
            if 'confidence' in col:
                cleaned_df[col] = cleaned_df[col].fillna(0.5)  # Confiance par d√©faut
            else:
                cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].median())
    
    return cleaned_df

# ================= ING√âNIERIE DE CARACT√âRISTIQUES POUR GENRE & √ÇGE =================
def create_gender_age_features(df):
    """Cr√©er des caract√©ristiques optimis√©es pour la pr√©diction du genre et de l'√¢ge"""
    print("üîß Cr√©ation de caract√©ristiques sp√©cifiques Genre & √Çge...")
    
    # D'abord, nettoyer les colonnes num√©riques
    numeric_columns = ['age', 'gender_confidence', 'race_confidence', 'emotion_confidence',
                      'bbox_w', 'bbox_h', 'bbox_x', 'bbox_y', 'face_area']
    
    enhanced_df = clean_numeric_columns(df, numeric_columns)
    
    # S'assurer que les colonnes requises existent
    required_cols = ['gender_confidence', 'race_confidence', 'emotion_confidence', 'age']
    for col in required_cols:
        if col not in enhanced_df.columns:
            enhanced_df[col] = 0.5
        enhanced_df[col] = pd.to_numeric(enhanced_df[col], errors='coerce').fillna(0.5)
    
    try:
        # Caract√©ristiques sp√©cifiques au genre
        enhanced_df['gender_conf_squared'] = enhanced_df['gender_confidence'] ** 2
        enhanced_df['gender_conf_log'] = np.log(enhanced_df['gender_confidence'] + 0.001)
        enhanced_df['high_gender_conf'] = (enhanced_df['gender_confidence'] > 0.8).astype(int)
        
        print("‚úÖ Caract√©ristiques de genre cr√©√©es")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la cr√©ation des caract√©ristiques de genre : {e}")
        enhanced_df['gender_conf_squared'] = 0.25
        enhanced_df['gender_conf_log'] = -1.0
        enhanced_df['high_gender_conf'] = 0
    
    try:
        # Caract√©ristiques sp√©cifiques √† l'√¢ge
        enhanced_df['age'] = pd.to_numeric(enhanced_df['age'], errors='coerce').fillna(30.0)
        enhanced_df['age_squared'] = enhanced_df['age'] ** 2
        enhanced_df['age_cubed'] = enhanced_df['age'] ** 3
        enhanced_df['age_log'] = np.log(enhanced_df['age'] + 1)
        enhanced_df['age_sqrt'] = np.sqrt(enhanced_df['age'])
        
        # Indicateurs de groupe d'√¢ge
        enhanced_df['is_child'] = (enhanced_df['age'] < 13).astype(int)
        enhanced_df['is_teen'] = ((enhanced_df['age'] >= 13) & (enhanced_df['age'] < 20)).astype(int)
        enhanced_df['is_young_adult'] = ((enhanced_df['age'] >= 20) & (enhanced_df['age'] < 30)).astype(int)
        enhanced_df['is_adult'] = ((enhanced_df['age'] >= 30) & (enhanced_df['age'] < 45)).astype(int)
        enhanced_df['is_middle_aged'] = ((enhanced_df['age'] >= 45) & (enhanced_df['age'] < 65)).astype(int)
        enhanced_df['is_senior'] = (enhanced_df['age'] >= 65).astype(int)
        
        print("‚úÖ Caract√©ristiques d'√¢ge cr√©√©es")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la cr√©ation des caract√©ristiques d'√¢ge : {e}")
        for feature in ['age_squared', 'age_cubed', 'age_log', 'age_sqrt']:
            enhanced_df[feature] = 900.0
        for feature in ['is_child', 'is_teen', 'is_young_adult', 'is_adult', 'is_middle_aged', 'is_senior']:
            enhanced_df[feature] = 0
    
    try:
        # Caract√©ristiques g√©om√©triques du visage (utiles pour genre/√¢ge)
        if 'bbox_w' in enhanced_df.columns and 'bbox_h' in enhanced_df.columns:
            enhanced_df['bbox_w'] = pd.to_numeric(enhanced_df['bbox_w'], errors='coerce').fillna(100)
            enhanced_df['bbox_h'] = pd.to_numeric(enhanced_df['bbox_h'], errors='coerce').fillna(100)
            
            enhanced_df['face_aspect_ratio'] = enhanced_df['bbox_w'] / (enhanced_df['bbox_h'] + 1)
            enhanced_df['face_area_norm'] = enhanced_df['bbox_w'] * enhanced_df['bbox_h']
            enhanced_df['face_perimeter'] = 2 * (enhanced_df['bbox_w'] + enhanced_df['bbox_h'])
            enhanced_df['face_compactness'] = (4 * np.pi * enhanced_df['face_area_norm']) / (enhanced_df['face_perimeter'] ** 2)
            
            print("‚úÖ Caract√©ristiques g√©om√©triques cr√©√©es")
        else:
            enhanced_df['face_aspect_ratio'] = 1.0
            enhanced_df['face_area_norm'] = 10000.0
            enhanced_df['face_perimeter'] = 400.0
            enhanced_df['face_compactness'] = 0.785
            print("‚ö†Ô∏è Utilisation de valeurs g√©om√©triques par d√©faut")
            
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la cr√©ation des caract√©ristiques g√©om√©triques : {e}")
        enhanced_df['face_aspect_ratio'] = 1.0
        enhanced_df['face_area_norm'] = 10000.0
        enhanced_df['face_perimeter'] = 400.0
        enhanced_df['face_compactness'] = 0.785
    
    try:
        # Interactions crois√©es entre caract√©ristiques
        enhanced_df['age_gender_conf'] = enhanced_df['age'] * enhanced_df['gender_confidence']
        enhanced_df['conf_diversity'] = enhanced_df['gender_confidence'] * enhanced_df['race_confidence'] * enhanced_df['emotion_confidence']
        enhanced_df['conf_avg'] = (enhanced_df['gender_confidence'] + enhanced_df['race_confidence'] + enhanced_df['emotion_confidence']) / 3
        
        print("‚úÖ Caract√©ristiques d'interaction cr√©√©es")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la cr√©ation des caract√©ristiques d'interaction : {e}")
        enhanced_df['age_gender_conf'] = 15.0
        enhanced_df['conf_diversity'] = 0.125
        enhanced_df['conf_avg'] = 0.5
    
    print(f"üìä Caract√©ristiques am√©lior√©es cr√©√©es. Nouvelle forme : {enhanced_df.shape}")
    return enhanced_df

# Appliquer l'ing√©nierie de caract√©ristiques
df = create_gender_age_features(df)

# ================= D√âFINIR LES CARACT√âRISTIQUES =================
# Caract√©ristiques de base
base_features = ['age', 'gender_confidence', 'race_confidence', 'emotion_confidence']

# Caract√©ristiques sp√©cifiques au genre
gender_features = ['gender_conf_squared', 'gender_conf_log', 'high_gender_conf']

# Caract√©ristiques sp√©cifiques √† l'√¢ge
age_features = ['age_squared', 'age_cubed', 'age_log', 'age_sqrt', 
               'is_child', 'is_teen', 'is_young_adult', 'is_adult', 'is_middle_aged', 'is_senior']

# Caract√©ristiques g√©om√©triques
geometry_features = ['face_aspect_ratio', 'face_area_norm', 'face_perimeter', 'face_compactness']

# Caract√©ristiques d'interaction
interaction_features = ['age_gender_conf', 'conf_diversity', 'conf_avg']

# Toutes les caract√©ristiques
all_features = base_features + gender_features + age_features + geometry_features + interaction_features

# V√©rifier quelles caract√©ristiques sont disponibles
available_features = [f for f in all_features if f in df.columns]
print(f"üìä Total des caract√©ristiques : {len(available_features)} sur {len(all_features)}")

# ================= NETTOYAGE DES DONN√âES =================
print("üßπ Nettoyage final des donn√©es...")

# Supprimer les lignes avec des √©tiquettes cibles manquantes
for label in target_labels:
    initial_count = len(df)
    df = df.dropna(subset=[label])
    removed = initial_count - len(df)
    if removed > 0:
        print(f"üßπ Supprim√© {removed} lignes avec {label} manquant")

# Cr√©er la matrice de caract√©ristiques
X = df[available_features].copy()

# Nettoyage final
for col in X.columns:
    X[col] = pd.to_numeric(X[col], errors='coerce')

X = X.fillna(X.median())
X = X.replace([np.inf, -np.inf], 0)

print(f"‚úÖ Forme finale du dataset : {X.shape}")
print(f"üéØ Caract√©ristiques utilis√©es : {len(X.columns)} caract√©ristiques")

# ================= FONCTIONS D'√âVALUATION =================
def evaluate_classifier_detailed(X_train, X_test, y_train, y_test, clf, label_name):
    """√âvaluation am√©lior√©e avec plusieurs m√©triques"""
    
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    
    try:
        y_pred_proba = clf.predict_proba(X_test)
    except:
        y_pred_proba = None
    
    # Calculer les m√©triques
    metrics = {
        "accuracy": accuracy_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred, average='weighted', zero_division=0),
        "recall": recall_score(y_test, y_pred, average='weighted', zero_division=0),
        "f1_weighted": f1_score(y_test, y_pred, average='weighted', zero_division=0),
        "f1_macro": f1_score(y_test, y_pred, average='macro', zero_division=0),
    }
    
    # ROC AUC pour binaire/multi-classes
    if y_pred_proba is not None:
        try:
            if len(np.unique(y_test)) == 2:
                metrics["roc_auc"] = roc_auc_score(y_test, y_pred_proba[:, 1])
            else:
                metrics["roc_auc"] = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')
        except:
            metrics["roc_auc"] = 0
    else:
        metrics["roc_auc"] = 0
    
    return metrics, y_pred, y_pred_proba

# ================= FONCTIONS DE VISUALISATION =================
def plot_class_distribution(y, title, label_encoder=None):
    """Tracer la distribution des classes"""
    plt.figure(figsize=(12, 6))
    
    if label_encoder:
        labels = label_encoder.classes_
        counts = pd.Series(y).value_counts()
        label_counts = [counts.get(i, 0) for i in range(len(labels))]
        bars = plt.bar(range(len(labels)), label_counts)
        plt.xticks(range(len(labels)), labels, rotation=45)
        
        # Ajouter des √©tiquettes de pourcentage sur les barres
        total = sum(label_counts)
        for bar, count in zip(bars, label_counts):
            percentage = (count / total) * 100 if total > 0 else 0
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(label_counts)*0.01, 
                    f'{count}\n({percentage:.1f}%)', ha='center', va='bottom')
    else:
        counts = pd.Series(y).value_counts()
        bars = counts.plot(kind='bar')
        plt.xticks(rotation=45)
        
        # Ajouter des √©tiquettes de pourcentage
        total = len(y)
        for i, (idx, count) in enumerate(counts.items()):
            percentage = (count / total) * 100
            plt.text(i, count + max(counts)*0.01, f'{count}\n({percentage:.1f}%)', 
                    ha='center', va='bottom')
    
    plt.title(f'{title} - Distribution des Classes')
    plt.xlabel('Classes')
    plt.ylabel('Nombre')
    plt.tight_layout()
    plt.show()

def plot_confusion_matrix_enhanced(y_true, y_pred, classes, title):
    """Tracer la matrice de confusion am√©lior√©e avec pourcentages"""
    cm = confusion_matrix(y_true, y_pred)
    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
    
    # Nombres
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=classes, yticklabels=classes, ax=ax1)
    ax1.set_title(f'{title} - Matrice de Confusion (Nombres)')
    ax1.set_xlabel('Pr√©dit')
    ax1.set_ylabel('R√©el')
    
    # Pourcentages
    sns.heatmap(cm_percentage, annot=True, fmt='.1f', cmap='Oranges', 
                xticklabels=classes, yticklabels=classes, ax=ax2)
    ax2.set_title(f'{title} - Matrice de Confusion (Pourcentages)')
    ax2.set_xlabel('Pr√©dit')
    ax2.set_ylabel('R√©el')
    
    plt.tight_layout()
    plt.show()

def plot_metrics_comparison(results_dict, title):
    """Tracer la comparaison compl√®te des m√©triques"""
    models = list(results_dict.keys())
    metrics = ['accuracy', 'precision', 'recall', 'f1_weighted', 'f1_macro', 'roc_auc']
    
    fig, axes = plt.subplots(2, 3, figsize=(20, 12))
    axes = axes.flatten()
    
    colors = plt.cm.Set3(np.linspace(0, 1, len(models)))
    
    for i, metric in enumerate(metrics):
        values = [results_dict[model][metric] * 100 for model in models]
        bars = axes[i].bar(models, values, color=colors)
        axes[i].set_title(f'{metric.upper().replace("_", " ")} (%)', fontsize=14, fontweight='bold')
        axes[i].set_ylabel('Pourcentage (%)')
        axes[i].set_ylim(0, 105)
        
        # Ajouter des √©tiquettes de valeur sur les barres
        for bar, value in zip(bars, values):
            axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                        f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        axes[i].tick_params(axis='x', rotation=45)
        axes[i].grid(True, alpha=0.3)
    
    plt.suptitle(f'{title} - Performance Compl√®te des Mod√®les', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()

# ================= R√âGLAGE DES HYPERPARAM√àTRES =================
def tune_hyperparameters(X_train, y_train, clf_name, clf):
    """R√©glage des hyperparam√®tres pour des classificateurs sp√©cifiques"""
    
    param_grids = {
        'RandomForest': {
            'n_estimators': [200, 300],
            'max_depth': [10, 15, None],
            'min_samples_split': [2, 3]
        },
        'GradientBoosting': {
            'n_estimators': [200, 300],
            'learning_rate': [0.05, 0.08, 0.1],
            'max_depth': [6, 8]
        },
        'SVM': {
            'C': [1, 10, 100],
            'kernel': ['rbf'],
            'gamma': ['scale', 'auto']
        },
        'LogisticRegression': {
            'C': [0.1, 1, 10],
            'solver': ['lbfgs', 'liblinear']
        }
    }
    
    if clf_name not in param_grids:
        return clf
    
    print(f"üîß R√©glage de {clf_name}...")
    
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    grid_search = GridSearchCV(
        clf, param_grids[clf_name], 
        cv=cv, scoring='f1_weighted', 
        n_jobs=-1, verbose=0
    )
    
    grid_search.fit(X_train, y_train)
    print(f"‚úÖ Meilleurs param√®tres : {grid_search.best_params_}")
    
    return grid_search.best_estimator_

# ================= √âVALUATION PRINCIPALE =================
def evaluate_gender_age():
    """Fonction d'√©valuation principale pour le genre et l'√¢ge"""
    
    all_results = {}
    
    for label in target_labels:
        print(f"\n{'='*70}")
        print(f"üéØ √âVALUATION : {label.upper()}")
        print(f"{'='*70}")
        
        # Pr√©parer la cible
        y = df[label].dropna()
        X_filtered = X.loc[y.index]
        
        # Encoder les √©tiquettes
        le = LabelEncoder()
        y_encoded = le.fit_transform(y.astype(str))
        
        print(f"üìä Classes dans {label} :")
        class_counts = pd.Series(y).value_counts()
        for cls, count in class_counts.items():
            percentage = (count / len(y)) * 100
            print(f"   - {cls} : {count} √©chantillons ({percentage:.1f}%)")
        
        # Tracer la distribution
        plot_class_distribution(y_encoded, f'{label.capitalize()}', le)
        
        # Division entra√Ænement-test
        min_class_count = min(pd.Series(y_encoded).value_counts())
        stratify = y_encoded if min_class_count >= 2 else None
        
        X_train, X_test, y_train, y_test = train_test_split(
            X_filtered, y_encoded, test_size=0.25, random_state=42, stratify=stratify
        )
        
        print(f"üìà Entra√Ænement : {len(X_train)}, Test : {len(X_test)}")
        
        # Normaliser les caract√©ristiques
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # Trouver la meilleure strat√©gie d'√©chantillonnage
        best_sampling = None
        best_score = 0
        
        print("\nüîç Test des strat√©gies d'√©chantillonnage...")
        for sampling_name, sampler in SAMPLING_STRATEGIES.items():
            try:
                X_resampled, y_resampled = sampler.fit_resample(X_train_scaled, y_train)
                rf = RandomForestClassifier(n_estimators=50, random_state=42)
                scores = cross_val_score(rf, X_resampled, y_resampled, cv=3, scoring='f1_weighted')
                avg_score = scores.mean()
                print(f"   - {sampling_name} : {avg_score:.3f}")
                
                if avg_score > best_score:
                    best_score = avg_score
                    best_sampling = sampling_name
            except Exception as e:
                print(f"   - {sampling_name} : √âchec")
        
        # Appliquer le meilleur √©chantillonnage
        if best_sampling:
            print(f"‚úÖ Utilisation : {best_sampling}")
            sampler = SAMPLING_STRATEGIES[best_sampling]
            X_train_final, y_train_final = sampler.fit_resample(X_train_scaled, y_train)
        else:
            X_train_final, y_train_final = X_train_scaled, y_train
            print("‚ö†Ô∏è Aucun √©chantillonnage appliqu√©")
        
        # √âvaluer tous les classificateurs
        results = {}
        predictions = {}
        
        print(f"\nüöÄ √âvaluation des classificateurs...")
        
        for clf_name, clf in CLASSIFIERS.items():
            print(f"\n--- {clf_name} ---")
            
            try:
                # R√©glage des hyperparam√®tres
                tuned_clf = tune_hyperparameters(X_train_final, y_train_final, clf_name, clf)
                
                # √âvaluer
                metrics, y_pred, y_pred_proba = evaluate_classifier_detailed(
                    X_train_final, X_test_scaled, y_train_final, y_test, tuned_clf, label
                )
                
                results[clf_name] = metrics
                predictions[clf_name] = (y_pred, y_pred_proba)
                
                # Afficher les r√©sultats
                print(f"Pr√©cision :    {metrics['accuracy']*100:.2f}%")
                print(f"Pr√©cision :   {metrics['precision']*100:.2f}%")
                print(f"Rappel :      {metrics['recall']*100:.2f}%")
                print(f"F1-Pond√©r√© : {metrics['f1_weighted']*100:.2f}%")
                print(f"F1-Macro :    {metrics['f1_macro']*100:.2f}%")
                print(f"ROC-AUC :     {metrics['roc_auc']*100:.2f}%")
                
                # Validation crois√©e
                cv_scores = cross_val_score(tuned_clf, X_train_final, y_train_final, 
                                          cv=5, scoring='f1_weighted')
                print(f"CV F1 :       {cv_scores.mean()*100:.2f}% ¬± {cv_scores.std()*100:.2f}%")
                
            except Exception as e:
                print(f"‚ùå Erreur : {e}")
                results[clf_name] = {
                    'accuracy': 0, 'precision': 0, 'recall': 0, 
                    'f1_weighted': 0, 'f1_macro': 0, 'roc_auc': 0
                }
        
        # Trouver le meilleur mod√®le
        best_f1 = max(results.values(), key=lambda x: x['f1_weighted'])['f1_weighted']
        best_models = [name for name, metrics in results.items() 
                      if metrics['f1_weighted'] == best_f1]
        
        print(f"\nüèÜ MEILLEUR MOD√àLE : {', '.join(best_models)}")
        print(f"üéØ Meilleur F1-Score : {best_f1*100:.2f}%")
        
        # Analyse d√©taill√©e pour le meilleur mod√®le
        best_model = best_models[0]
        best_y_pred, best_y_pred_proba = predictions[best_model]
        
        print(f"\nüìã Rapport de Classification ({best_model}) :")
        report = classification_report(y_test, best_y_pred, 
                                     target_names=le.classes_, zero_division=0)
        print(report)
        
        # Visualisations
        plot_confusion_matrix_enhanced(y_test, best_y_pred, le.classes_, 
                                     f'{label.capitalize()} - {best_model}')
        
        plot_metrics_comparison(results, f'{label.capitalize()}')
        
        # Importance des caract√©ristiques
        if best_model in ['RandomForest', 'GradientBoosting', 'AdaBoost']:
            tuned_clf = tune_hyperparameters(X_train_final, y_train_final, best_model, 
                                           CLASSIFIERS[best_model])
            tuned_clf.fit(X_train_final, y_train_final)
            
            if hasattr(tuned_clf, 'feature_importances_'):
                importance_df = pd.DataFrame({
                    'feature': X.columns,
                    'importance': tuned_clf.feature_importances_
                }).sort_values('importance', ascending=False)
                
                print(f"\nüîç Top 15 des Caract√©ristiques Importantes ({best_model}) :")
                for _, row in importance_df.head(15).iterrows():
                    print(f"   {row['feature']} : {row['importance']:.4f}")
                
                # Tracer l'importance des caract√©ristiques
                plt.figure(figsize=(12, 10))
                top_features = importance_df.head(20)
                sns.barplot(data=top_features, y='feature', x='importance', palette='viridis')
                plt.title(f'{label.capitalize()} - Top 20 Importance des Caract√©ristiques ({best_model})')
                plt.xlabel('Score d\'Importance')
                plt.tight_layout()
                plt.show()
        
        # Stocker les r√©sultats
        all_results[label] = {
            'best_model': best_model,
            'best_score': best_f1,
            'all_results': results,
            'class_distribution': class_counts.to_dict(),
            'sampling_strategy': best_sampling
        }
    
    return all_results

# ================= RAPPORT R√âCAPITULATIF =================
def generate_final_report(all_results):
    """G√©n√©rer un rapport final complet"""
    
    print(f"\n{'='*80}")
    print(f"üìä CLASSIFICATION GENRE & √ÇGE - RAPPORT FINAL")
    print(f"{'='*80}")
    
    summary_data = []
    
    for label, results in all_results.items():
        summary_data.append({
            'Cible': label.capitalize(),
            'Meilleur Mod√®le': results['best_model'],
            'F1-Score': f"{results['best_score']*100:.2f}%",
            'Classes': len(results['class_distribution']),
            '√âchantillonnage': results['sampling_strategy'] or 'Aucun'
        })
        
        print(f"\nüéØ R√âSULTATS {label.upper()} :")
        print(f"   - Meilleur Mod√®le : {results['best_model']}")
        print(f"   - F1-Score : {results['best_score']*100:.2f}%")
        print(f"   - Classes : {len(results['class_distribution'])}")
        print(f"   - √âchantillonnage : {results['sampling_strategy'] or 'Aucun'}")
        
        # Performance par classe
        print(f"   - Distribution des Classes :")
        for class_name, count in results['class_distribution'].items():
            total = sum(results['class_distribution'].values())
            percentage = (count / total) * 100
            print(f"     * {class_name} : {count} ({percentage:.1f}%)")
        
        # Comparaison des mod√®les
        print(f"   - Performance de Tous les Mod√®les :")
        sorted_models = sorted(results['all_results'].items(), 
                             key=lambda x: x[1]['f1_weighted'], reverse=True)
        for model_name, metrics in sorted_models:
            print(f"     * {model_name} : {metrics['f1_weighted']*100:.1f}%")
    
    # Cr√©er le tableau r√©capitulatif
    summary_df = pd.DataFrame(summary_data)
    print(f"\nüìã TABLEAU R√âCAPITULATIF :")
    print("+" + "-"*70 + "+")
    print(f"| {'Cible':<12} | {'Meilleur Mod√®le':<15} | {'F1-Score':<10} | {'Classes':<7} | {'√âchantillonnage':<12} |")
    print("+" + "-"*70 + "+")
    for _, row in summary_df.iterrows():
        print(f"| {row['Cible']:<12} | {row['Meilleur Mod√®le']:<15} | {row['F1-Score']:<10} | {row['Classes']:<7} | {row['√âchantillonnage']:<12} |")
    print("+" + "-"*70 + "+")
    
    # Insights globaux
    print(f"\nüí° INSIGHTS CL√âS :")
    
    # Meilleurs mod√®les globalement
    all_models = {}
    for results in all_results.values():
        for model, metrics in results['all_results'].items():
            if model not in all_models:
                all_models[model] = []
            all_models[model].append(metrics['f1_weighted'])
    
    avg_performance = {model: np.mean(scores) for model, scores in all_models.items()}
    best_overall = max(avg_performance, key=avg_performance.get)
    
    print(f"   - Meilleur Mod√®le Global : {best_overall} (F1 Moyen : {avg_performance[best_overall]*100:.1f}%)")
    
    # Comparaison performance Genre vs √Çge
    gender_score = all_results.get('gender', {}).get('best_score', 0) * 100
    age_score = all_results.get('age_category', {}).get('best_score', 0) * 100
    
    if gender_score > 0 and age_score > 0:
        if gender_score > age_score:
            print(f"   - La classification du Genre performe mieux que l'√Çge ({gender_score:.1f}% vs {age_score:.1f}%)")
        else:
            print(f"   - La classification de l'√Çge performe mieux que le Genre ({age_score:.1f}% vs {gender_score:.1f}%)")
        
        diff = abs(gender_score - age_score)
        if diff < 5:
            print(f"   - Diff√©rence de performance minimale ({diff:.1f}%)")
        elif diff < 15:
            print(f"   - Diff√©rence de performance mod√©r√©e ({diff:.1f}%)")
        else:
            print(f"   - Diff√©rence de performance significative ({diff:.1f}%)")
    
    # Insights sur les caract√©ristiques
    print(f"\nüîç INSIGHTS SUR LES CARACT√âRISTIQUES :")
    print(f"   - Total des Caract√©ristiques Utilis√©es : {len(available_features)}")
    print(f"   - Cat√©gories de Caract√©ristiques :")
    print(f"     * Caract√©ristiques de Base : {len(base_features)} (√¢ge, confiances)")
    print(f"     * Caract√©ristiques de Genre : {len(gender_features)} (sp√©cifiques au genre)")
    print(f"     * Caract√©ristiques d'√Çge : {len(age_features)} (transformations & indicateurs d'√¢ge)")
    print(f"     * Caract√©ristiques G√©om√©triques : {len(geometry_features)} (mesures du visage)")
    print(f"     * Caract√©ristiques d'Interaction : {len(interaction_features)} (caract√©ristiques crois√©es)")
    
    # Recommandations
    print(f"\nüöÄ RECOMMANDATIONS :")
    
    for label, results in all_results.items():
        score = results['best_score'] * 100
        if score >= 90:
            print(f"   - {label.capitalize()} : Performance excellente ({score:.1f}%) - Pr√™t pour la production")
        elif score >= 80:
            print(f"   - {label.capitalize()} : Bonne performance ({score:.1f}%) - Envisager l'optimisation")
        elif score >= 70:
            print(f"   - {label.capitalize()} : Performance mod√©r√©e ({score:.1f}%) - N√©cessite am√©lioration")
        else:
            print(f"   - {label.capitalize()} : Performance faible ({score:.1f}%) - N√©cessite travail donn√©es/caract√©ristiques")
    
    # Recommandations qualit√© des donn√©es
    total_samples = len(df)
    if total_samples < 1000:
        print(f"   - Collecter plus de donn√©es : {total_samples} √©chantillons actuels peuvent √™tre insuffisants")
    
    # Recommandations d√©s√©quilibre des classes
    for label, results in all_results.items():
        class_dist = results['class_distribution']
        max_class = max(class_dist.values())
        min_class = min(class_dist.values())
        imbalance_ratio = max_class / min_class if min_class > 0 else float('inf')
        
        if imbalance_ratio > 10:
            print(f"   - {label.capitalize()} : D√©s√©quilibre √©lev√© des classes (ratio : {imbalance_ratio:.1f}) - Consid√©rer l'augmentation de donn√©es")
        elif imbalance_ratio > 5:
            print(f"   - {label.capitalize()} : D√©s√©quilibre mod√©r√© des classes (ratio : {imbalance_ratio:.1f}) - L'√©chantillonnage a aid√©")
    
    return summary_df

# ================= EX√âCUTION PRINCIPALE =================
if __name__ == "__main__":
    print("üöÄ D√©marrage du Syst√®me d'√âvaluation Classification Genre & √Çge")
    print(f"üìä Dataset : {EXCEL_PATH}")
    print(f"üéØ √âtiquettes Cibles : Genre & Cat√©gorie d'√Çge")
    print(f"ü§ñ Classificateurs : {len(CLASSIFIERS)} mod√®les")
    print(f"‚öñÔ∏è Strat√©gies d'√âchantillonnage : {len(SAMPLING_STRATEGIES)} m√©thodes")
    print(f"üìà Caract√©ristiques : Caract√©ristiques compl√®tes sp√©cifiques au genre & √¢ge")
    
    # Aper√ßu des donn√©es
    print(f"\nüìã APER√áU DES DONN√âES :")
    print(f"   - Total d'√âchantillons : {len(df)}")
    print(f"   - Total de Caract√©ristiques : {len(available_features)}")
    
    # Distribution du genre
    if 'gender' in df.columns:
        gender_dist = df['gender'].value_counts()
        print(f"   - Distribution du Genre :")
        for gender, count in gender_dist.items():
            percentage = (count / len(df)) * 100
            print(f"     * {gender} : {count} ({percentage:.1f}%)")
    
    # Distribution de l'√¢ge
    if 'age_category' in df.columns:
        age_dist = df['age_category'].value_counts()
        print(f"   - Distribution des Cat√©gories d'√Çge :")
        for age_cat, count in age_dist.items():
            percentage = (count / len(df)) * 100
            print(f"     * {age_cat} : {count} ({percentage:.1f}%)")
    
    print(f"\n{'='*60}")
    print("üèÅ D√âMARRAGE DE L'√âVALUATION...")
    print(f"{'='*60}")
    
    # Ex√©cuter l'√©valuation
    results = evaluate_gender_age()
    
    # G√©n√©rer le rapport final
    summary = generate_final_report(results)
    
    print(f"\n‚úÖ √âVALUATION TERMIN√âE AVEC SUCC√àS !")
    print(f"üìä Consultez les visualisations et rapports g√©n√©r√©s ci-dessus")
    print(f"üíæ Donn√©es r√©capitulatives disponibles dans le DataFrame retourn√©")
    print(f"\nüéâ Merci d'avoir utilis√© le Syst√®me de Classification Genre & √Çge !")